

# FAWO-MVSNet: Multi-View 3D Reconstruction Based on Feature Enhancement and Weight Optimization

## ðŸ“” Introduction
In this paper, we present TransMVSNet, based on our exploration of feature matching in multi-view stereo (MVS). We analogize MVS back to its nature of a feature matching task and therefore propose a powerful Feature Matching Transformer (FMT) to leverage intra- (self-) and inter- (cross-) attention to aggregate long-range context information within and across images. To facilitate a better adaptation of the FMT, we leverage an Adaptive Receptive Field (ARF) module to ensure a smooth transit in scopes of features and bridge different stages with a feature pathway to pass transformed features and gradients across different scales. In addition, we apply pair-wise feature correlation to measure similarity between features, and adopt ambiguity-reducing focal loss to strengthen the supervision. To the best of our knowledge, TransMVSNet is the first attempt to leverage Transformer into the task of MVS. As a result, our method achieves state-of-the-art performance on DTU dataset, Tanks and Temples benchmark, and BlendedMVS dataset.
![](assets/overview.png)

## ðŸ“¦ Data preparation
In FAWO-MVSNet, we mainly use [DTU](https://roboimagedata.compute.dtu.dk/), [BlendedMVS](https://github.com/YoYo000/BlendedMVS/) and [Tanks and Temples](https://www.tanksandtemples.org/) to train and evaluate our models. You can prepare the corresponding data by following the instructions below.

### âœ”  DTU
For DTU training set, you can download the preprocessed [DTU training data](https://drive.google.com/file/d/1eDjh-_bxKKnEuz5h-HXS7EDJn59clx6V/view)
 and [Depths_raw](https://virutalbuy-public.oss-cn-hangzhou.aliyuncs.com/share/cascade-stereo/CasMVSNet/dtu_data/dtu_train_hr/Depths_raw.zip)
 (both from [Original MVSNet](https://github.com/YoYo000/MVSNet)), and unzip them to construct a dataset folder like:
```
dtu_training
 â”œâ”€â”€ Cameras
 â”œâ”€â”€ Depths
 â”œâ”€â”€ Depths_raw
 â””â”€â”€ Rectified
```
For DTU testing set, you can download the preprocessed [DTU testing data](https://drive.google.com/open?id=135oKPefcPTsdtLRzoDAQtPpHuoIrpRI_) (from [Original MVSNet](https://github.com/YoYo000/MVSNet)) and unzip it as the test data folder, which should contain one ``cams`` folder, one ``images`` folder and one ``pair.txt`` file.

### âœ”  BlendedMVS
We use the [low-res set](https://1drv.ms/u/s!Ag8Dbz2Aqc81gVDgxb8MDGgoV74S?e=hJKlvV) of BlendedMVS dataset for both training and testing. You can download the [low-res set](https://1drv.ms/u/s!Ag8Dbz2Aqc81gVDgxb8MDGgoV74S?e=hJKlvV) from [orignal BlendedMVS](https://github.com/YoYo000/BlendedMVS) and unzip it to form the dataset folder like below:

```
BlendedMVS
 â”œâ”€â”€ 5a0271884e62597cdee0d0eb
 â”‚     â”œâ”€â”€ blended_images
 â”‚     â”œâ”€â”€ cams
 â”‚     â””â”€â”€ rendered_depth_maps
 â”œâ”€â”€ 59338e76772c3e6384afbb15
 â”œâ”€â”€ 59f363a8b45be22330016cad
 â”œâ”€â”€ ...
 â”œâ”€â”€ all_list.txt
 â”œâ”€â”€ training_list.txt
 â””â”€â”€ validation_list.txt
```

### âœ”  Tanks and Temples
Download our preprocessed [Tanks and Temples dataset](https://drive.google.com/file/d/1IHG5GCJK1pDVhDtTHFS3sY-ePaK75Qzg/view?usp=sharing) and unzip it to form the dataset folder like below:
```
tankandtemples
 â”œâ”€â”€ advanced
 â”‚  â”œâ”€â”€ Auditorium
 â”‚  â”œâ”€â”€ Ballroom
 â”‚  â”œâ”€â”€ ...
 â”‚  â””â”€â”€ Temple
 â””â”€â”€ intermediate
        â”œâ”€â”€ Family
        â”œâ”€â”€ Francis
        â”œâ”€â”€ ...
        â””â”€â”€ Train
```
